{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Notes on MLFlow:\n",
    "- 'Run name' field: model name, type of output (multilabel vs unilabel), which vaccine (for multiclass only)\n",
    "- 'Parameters' field: methods applied for data (data cleaning, data balancing, hyperparameters)--insert feature engineering info here, if relevant?\n",
    "- 'Tags' field: details about the features used for the run (is one of the vaccines in the features?)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "#mlflow import\n",
    "import mlflow\n",
    "from modeling.config import EXPERIMENT_NAME_multilabel, EXPERIMENT_NAME_h1n1, EXPERIMENT_NAME_seasonal, EXPERIMENT_NAME_multiclass\n",
    "TRACKING_URI = open(\"../.mlflow_uri\").read().strip()\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "# Modeling Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "!pip install plotly\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# for resampling (data balancing)\n",
    "from sklearn.utils import resample\n",
    "!pip install -U imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import data from a previously prepared dataframe:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.read_csv('../data/Flu_Shot_Data_cleaned_2.csv')"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.head()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dropping the strange 'Unnamed: 0' column:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set up for modelling (stays the same for all experiemnts)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up of pipeline preprocessor:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Pipeline for categorical features\n",
    "# This stays the same for everything\n",
    "cat_pipeline = Pipeline([\n",
    "    ('1hot', OneHotEncoder(handle_unknown='error', drop='first'))\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features = list(df.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Instantiating the models:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# for KNN\n",
    "knn= KNeighborsClassifier()\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst= RandomForestClassifier()\n",
    "\n",
    "# for SVM\n",
    "svm= svm.SVC(kernel='rbf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TheFluShot_multilabel: Multilabel prediction (both vaccinations)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Removal of target variables from cat_features list (this needs to be adjusted for each dataset):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_no_vacc = cat_features.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_no_vacc.remove('h1n1_vaccine')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_no_vacc.remove('seasonal_vaccine')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_no_vacc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Rename the features and target to 'X' and 'y', to make the test-train split easier:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_both_vacc = df[['h1n1_vaccine', 'seasonal_vaccine']]#.copy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#y_both_vacc = y_both_vacc.to_numpy()\n",
    "y_both_vacc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#NB: dropping the 'h1n1_vaccine' and 'seasonal_vaccine' columns\n",
    "X_no_vacc = df.drop(columns=['h1n1_vaccine', 'seasonal_vaccine'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing test-train split (the same data can be used for each model in multilabelling):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_train, X_no_vacc_test, y_both_vacc_train, y_both_vacc_test = train_test_split(X_no_vacc, y_both_vacc, stratify = y_both_vacc, test_size=0.2, random_state=RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('X_no_vacc_train shape:', X_no_vacc_train.shape)\n",
    "print('X_no_vacc_test shape:', X_no_vacc_test.shape)\n",
    "print('y_both_vacc_train:', y_both_vacc_train.shape)\n",
    "print('y_both_vacc_test:', y_both_vacc_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the preprocessor (the same one can be used for each modelling in multilabelling):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, cat_features_no_vacc)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the multilabel estimators for each model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logistic regression\n",
    "multilabel_est_logreg = MultiOutputClassifier(\n",
    "    estimator=logreg\n",
    ")\n",
    "\n",
    "# for KNN\n",
    "multilabel_est_knn= MultiOutputClassifier(\n",
    "    estimator=knn\n",
    ")\n",
    "\n",
    "# for Random Forest\n",
    "multilabel_est_rand_forst= MultiOutputClassifier(\n",
    "    estimator=rand_forst\n",
    ")\n",
    "\n",
    "\n",
    "# for SVM\n",
    "multilabel_est_SVC= MultiOutputClassifier(\n",
    "    estimator=svm\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the pipeline for each model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_multilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", multilabel_est_logreg),\n",
    "])\n",
    "\n",
    "# for KNN\n",
    "knn_multilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", multilabel_est_knn),\n",
    "])\n",
    "\n",
    "# for Random Forest\n",
    "\n",
    "rand_forst_multilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", multilabel_est_rand_forst),\n",
    "])\n",
    "\n",
    "#for SVM\n",
    "svm_multilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", multilabel_est_SVC),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Balancing\n",
    "\n",
    "https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "Since the H1N1 label is unbalanced, three approaches will be tried to balance it:\n",
    "\n",
    "1. Oversampling of the minority class (h1n1_vaccine == 1)\n",
    "2. Undersampling of the majority class (h1n1_vaccine == 0)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating concatenated training dataframe and separating into minority and majority class  (for data balancing)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenating our train data back together\n",
    "\n",
    "concat_train_df = pd.concat([X_no_vacc_train, y_both_vacc_train], axis = 1)\n",
    "\n",
    "# separating into minority and majority classes\n",
    "\n",
    "# majority class\n",
    "no_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==0]\n",
    "\n",
    "# minority class\n",
    "yes_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yes_h1n1_vacc.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Oversampling of the minority class (upsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#upsample minority class\n",
    "yes_h1n1_vacc_upsampled = resample(yes_h1n1_vacc,\n",
    "                                   replace = True,\n",
    "                                   n_samples = len(no_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine majority and upsampled minority\n",
    "\n",
    "upsampled = pd.concat([no_h1n1_vacc, yes_h1n1_vacc_upsampled])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "upsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "upsampled.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the upsampled data as train dataset\n",
    "\n",
    "y_multi_vacc_upsamp_train = upsampled[['h1n1_vaccine', 'seasonal_vaccine']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_upsamp_train =upsampled.drop(['h1n1_vaccine','seasonal_vaccine'], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_upsamp_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Undersampling of majority class (downsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# downsample minority class\n",
    "no_h1n1_vacc_downsampled = resample(no_h1n1_vacc,\n",
    "                                   replace = False,\n",
    "                                   n_samples = len(yes_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine minority and downsampled majority\n",
    "\n",
    "downsampled = pd.concat([no_h1n1_vacc_downsampled, yes_h1n1_vacc])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "downsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the downsampled data as train dataset\n",
    "\n",
    "y_multi_vacc_downsamp_train = downsampled[['h1n1_vaccine', 'seasonal_vaccine']]\n",
    "\n",
    "X_no_vacc_downsamp_train =downsampled.drop(['h1n1_vaccine','seasonal_vaccine'], axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Convert target variable to array, otherwise the evaluation metrics fail:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_both_vacc_train = y_both_vacc_train.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_both_vacc_test = y_both_vacc_test.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training and predictions (run predictions immediately after training the model!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_multilabel = logreg_multilabel_pipeline.fit(X_no_vacc_train,  y_both_vacc_train)\n",
    "\n",
    "# for KNN\n",
    "knn_multilabel = knn_multilabel_pipeline.fit(X_no_vacc_train,  y_both_vacc_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_multilabel = rand_forst_multilabel_pipeline.fit(X_no_vacc_train,  y_both_vacc_train)\n",
    "\n",
    "#for SVM\n",
    "svm_multilabel = svm_multilabel_pipeline.fit(X_no_vacc_train,  y_both_vacc_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figure out later what this does and if we want to use it\n",
    "\n",
    "\n",
    "#y_train_predicted = cross_val_predict(full_pipeline, X_train, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_multilabel_trainpreds = logreg_multilabel.predict(X_no_vacc_train)\n",
    "logreg_multilabel_testpreds = logreg_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_multilabel_trainpreds = knn_multilabel.predict(X_no_vacc_train)\n",
    "knn_multilabel_testpreds = knn_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_multilabel_trainpreds = rand_forst_multilabel.predict(X_no_vacc_train)\n",
    "rand_forst_multilabel_testpreds = rand_forst_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_multilabel_trainpreds = svm_multilabel.predict(X_no_vacc_train)\n",
    "svm_multilabel_testpreds = svm_multilabel.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampling--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_upsamp_multilabel = logreg_multilabel_pipeline.fit(X_no_vacc_upsamp_train, y_multi_vacc_upsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_upsamp_multilabel = knn_multilabel_pipeline.fit(X_no_vacc_upsamp_train, y_multi_vacc_upsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_upsamp_multilabel = rand_forst_multilabel_pipeline.fit(X_no_vacc_upsamp_train, y_multi_vacc_upsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_upsamp_multilabel = svm_multilabel_pipeline.fit(X_no_vacc_upsamp_train, y_multi_vacc_upsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampling--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_upsamp_multilabel_trainpreds = logreg_upsamp_multilabel.predict(X_no_vacc_train)\n",
    "logreg_upsamp_multilabel_testpreds = logreg_upsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_upsamp_multilabel_trainpreds = knn_upsamp_multilabel.predict(X_no_vacc_train)\n",
    "knn_upsamp_multilabel_testpreds = knn_upsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_upsamp_multilabel_trainpreds = rand_forst_upsamp_multilabel.predict(X_no_vacc_train)\n",
    "rand_forst_upsamp_multilabel_testpreds = rand_forst_upsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_upsamp_multilabel_trainpreds = svm_upsamp_multilabel.predict(X_no_vacc_train)\n",
    "svm_upsamp_multilabel_testpreds = svm_upsamp_multilabel.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampling--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_downsamp_multilabel = logreg_multilabel_pipeline.fit(X_no_vacc_downsamp_train, y_multi_vacc_downsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_downsamp_multilabel = knn_multilabel_pipeline.fit(X_no_vacc_downsamp_train, y_multi_vacc_downsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_downsamp_multilabel = rand_forst_multilabel_pipeline.fit(X_no_vacc_downsamp_train, y_multi_vacc_downsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_downsamp_multilabel = svm_multilabel_pipeline.fit(X_no_vacc_downsamp_train, y_multi_vacc_downsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampling--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_downsamp_multilabel_trainpreds = logreg_downsamp_multilabel.predict(X_no_vacc_train)\n",
    "logreg_downsamp_multilabel_testpreds = logreg_downsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_downsamp_multilabel_trainpreds = knn_downsamp_multilabel.predict(X_no_vacc_train)\n",
    "knn_downsamp_multilabel_testpreds = knn_downsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_downsamp_multilabel_trainpreds = rand_forst_downsamp_multilabel.predict(X_no_vacc_train)\n",
    "rand_forst_downsamp_multilabel_testpreds = rand_forst_downsamp_multilabel.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_downsamp_multilabel_trainpreds = svm_downsamp_multilabel.predict(X_no_vacc_train)\n",
    "svm_downsamp_multilabel_testpreds = svm_downsamp_multilabel.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], logreg_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], logreg_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], logreg_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_multilabel_train_f0 = f0_score(y_both_vacc_train[:, 0], logreg_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], logreg_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 1], logreg_multilabel_trainpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 0], knn_multilabel_trainpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 1], knn_multilabel_trainpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics for H1N1 Vaccines\n",
    "# THESE RESULTS ARE SLIGHTLY OFF FROM PREVIOUS NOTEBOOK (BY 0.01)--CHECK WHAT'S AT PLAY. VARIATION COMING FROM TEST-TRAIN SPLIT? EXPORT TEST-TRAIN DATA AS CSV AND TRY AGAIN\n",
    "#IS IT BECAUSE IT'S RANDOM FOREST? DIFFERENT STARTING STUMP EACH TIME?\n",
    "h1n1_rand_forst_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 0], rand_forst_multilabel_trainpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data Evaluation Metrics for seasonal Vaccines\n",
    "# THESE RESULTS ARE SLIGHTLY OFF FROM PREVIOUS NOTEBOOK (BY 0.01)--CHECK WHAT'S AT PLAY. VARIATION COMING FROM TEST-TRAIN SPLIT? EXPORT TEST-TRAIN DATA AS CSV AND TRY AGAIN\n",
    "#IS IT BECAUSE IT'S RANDOM FOREST? DIFFERENT STARTING STUMP EACH TIME?\n",
    "seasonal_rand_forst_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 1], rand_forst_multilabel_trainpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 0], svm_multilabel_trainpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_train[:, 1], svm_multilabel_trainpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 0], logreg_multilabel_testpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 0], knn_multilabel_testpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 1], knn_multilabel_testpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics for H1N1 Vaccines\n",
    "# THESE RESULTS ARE SLIGHTLY OFF FROM PREVIOUS NOTEBOOK (BY 0.01)--CHECK WHAT'S AT PLAY. VARIATION COMING FROM TEST-TRAIN SPLIT? EXPORT TEST-TRAIN DATA AS CSV AND TRY AGAIN\n",
    "#IS IT BECAUSE IT'S RANDOM FOREST? DIFFERENT STARTING STUMP EACH TIME?\n",
    "h1n1_rand_forst_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 0], rand_forst_multilabel_testpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data Evaluation Metrics for seasonal Vaccines\n",
    "# THESE RESULTS ARE SLIGHTLY OFF FROM PREVIOUS NOTEBOOK (BY 0.01)--CHECK WHAT'S AT PLAY. VARIATION COMING FROM TEST-TRAIN SPLIT? EXPORT TEST-TRAIN DATA AS CSV AND TRY AGAIN\n",
    "#IS IT BECAUSE IT'S RANDOM FOREST? DIFFERENT STARTING STUMP EACH TIME?\n",
    "seasonal_rand_forst_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 1], rand_forst_multilabel_testpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 0], svm_multilabel_testpreds[:, 0])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], svm_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], svm_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], svm_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], svm_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], svm_multilabel_testpreds[:, 1])\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_both_vacc_test[:, 1], logreg_multilabel_testpreds[:, 1])))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_both_vacc_train = y_both_vacc_train.to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], logreg_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], logreg_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], logreg_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], logreg_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], logreg_upsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], logreg_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], logreg_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], logreg_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], logreg_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], logreg_upsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], knn_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], knn_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], knn_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], knn_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], knn_upsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], knn_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], knn_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], knn_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], knn_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], knn_upsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_rand_forst_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], rand_forst_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], rand_forst_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], rand_forst_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], rand_forst_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], rand_forst_upsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_rand_forst_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], rand_forst_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], rand_forst_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], rand_forst_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], rand_forst_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], rand_forst_upsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], svm_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], svm_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], svm_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], svm_upsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], svm_upsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_upsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], svm_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], svm_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], svm_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], svm_upsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], svm_upsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], logreg_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], logreg_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], logreg_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], logreg_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], logreg_upsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], logreg_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], logreg_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], logreg_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], logreg_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], logreg_upsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], knn_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], knn_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], knn_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], knn_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], knn_upsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], knn_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], knn_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], knn_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], knn_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], knn_upsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_rand_forst_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], rand_forst_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], rand_forst_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], rand_forst_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], rand_forst_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], rand_forst_upsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_rand_forst_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], rand_forst_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], rand_forst_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], rand_forst_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], rand_forst_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], rand_forst_upsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], svm_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], svm_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], svm_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], svm_upsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], svm_upsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_upsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], svm_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], svm_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], svm_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], svm_upsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_upsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], svm_upsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--downsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], logreg_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], logreg_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], logreg_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], logreg_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], logreg_downsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], logreg_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], logreg_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], logreg_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], logreg_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], logreg_downsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], knn_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], knn_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], knn_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], knn_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], knn_downsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], knn_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], knn_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], knn_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], knn_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], knn_downsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_rand_forst_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], rand_forst_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], rand_forst_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], rand_forst_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], rand_forst_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], rand_forst_downsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_rand_forst_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], rand_forst_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], rand_forst_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], rand_forst_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], rand_forst_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], rand_forst_downsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 0], svm_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 0], svm_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 0], svm_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 0], svm_downsamp_multilabel_trainpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 0], svm_downsamp_multilabel_trainpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_downsamp_multilabel_train_acc = accuracy_score(y_both_vacc_train[:, 1], svm_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_train_recall = recall_score(y_both_vacc_train[:, 1], svm_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_train_precision = precision_score(y_both_vacc_train[:, 1], svm_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_train_f1 = f1_score(y_both_vacc_train[:, 1], svm_downsamp_multilabel_trainpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_train_roc = roc_auc_score(y_both_vacc_train[:, 1], svm_downsamp_multilabel_trainpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--downsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_logreg_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], logreg_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], logreg_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], logreg_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], logreg_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_logreg_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], logreg_downsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_logreg_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], logreg_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], logreg_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], logreg_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], logreg_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_logreg_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], logreg_downsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_knn_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], knn_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], knn_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], knn_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], knn_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_knn_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], knn_downsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_knn_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], knn_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], knn_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], knn_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], knn_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_knn_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], knn_downsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_rand_forst_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], rand_forst_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], rand_forst_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], rand_forst_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], rand_forst_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_rand_forst_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], rand_forst_downsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random forest--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_rand_forst_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], rand_forst_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], rand_forst_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], rand_forst_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], rand_forst_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_rand_forst_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], rand_forst_downsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data Evaluation Metrics for H1N1 Vaccines\n",
    "h1n1_svm_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 0], svm_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 0], svm_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 0], svm_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 0], svm_downsamp_multilabel_testpreds[:, 0])\n",
    "h1n1_svm_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 0], svm_downsamp_multilabel_testpreds[:, 0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data Evaluation Metrics for seasonal Vaccines\n",
    "seasonal_svm_downsamp_multilabel_test_acc = accuracy_score(y_both_vacc_test[:, 1], svm_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_test_recall = recall_score(y_both_vacc_test[:, 1], svm_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_test_precision = precision_score(y_both_vacc_test[:, 1], svm_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_test_f1 = f1_score(y_both_vacc_test[:, 1], svm_downsamp_multilabel_testpreds[:, 1])\n",
    "seasonal_svm_downsamp_multilabel_test_roc = roc_auc_score(y_both_vacc_test[:, 1], svm_downsamp_multilabel_testpreds[:, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking the model with MLFlow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### H1N1 vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'knn_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'knn_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'knn_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'rand_forst_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'rand_forst_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'rand_forst_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'svm_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'svm_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seasonal vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'logreg_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_logreg_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_logreg_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_logreg_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_logreg_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_logreg_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_logreg_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_logreg_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_logreg_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_logreg_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_logreg_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_logreg_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_logreg_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_logreg_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_logreg_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_logreg_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_logreg_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_logreg_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_logreg_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_logreg_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_logreg_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_logreg_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_logreg_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_logreg_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_logreg_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_logreg_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_logreg_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_logreg_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_logreg_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_logreg_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_logreg_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'knn_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_knn_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_knn_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_knn_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_knn_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_knn_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" +m \"recall\", seasonal_knn_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_knn_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_knn_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_knn_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_knn_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'knn_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_knn_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_knn_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_knn_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_knn_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_knn_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_knn_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_knn_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_knn_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_knn_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_knn_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'knn_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_knn_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_knn_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_knn_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_knn_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_knn_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_knn_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_knn_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_knn_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_knn_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_knn_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'rand_forst_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_rand_forst_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_rand_forst_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_rand_forst_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_rand_forst_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_rand_forst_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_rand_forst_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_rand_forst_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_rand_forst_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_rand_forst_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_rand_forst_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'rand_forst_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_rand_forst_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_rand_forst_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_rand_forst_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_rand_forst_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_rand_forst_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_rand_forst_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_rand_forst_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_rand_forst_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_rand_forst_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_rand_forst_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'rand_forst_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_rand_forst_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_rand_forst_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_rand_forst_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_rand_forst_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_rand_forst_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_rand_forst_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_rand_forst_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_rand_forst_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_rand_forst_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_rand_forst_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel)\n",
    "name = 'svm_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_svm_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_svm_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_svm_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_svm_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_svm_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_svm_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_svm_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_svm_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_svm_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_svm_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'svm_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_svm_upsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_svm_upsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_svm_upsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_svm_upsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_svm_upsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_svm_upsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_svm_upsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_svm_upsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_svm_upsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_svm_upsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_multilabel) #this needs to be adjusted for each experiment\n",
    "name = 'svm_multilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))\n",
    "\n",
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling H1N1\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }\n",
    "\n",
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seasonal_svm_downsamp_multilabel_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seasonal_svm_downsamp_multilabel_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seasonal_svm_downsamp_multilabel_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seasonal_svm_downsamp_multilabel_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seasonal_svm_downsamp_multilabel_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seasonal_svm_downsamp_multilabel_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seasonal_svm_downsamp_multilabel_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seasonal_svm_downsamp_multilabel_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seasonal_svm_downsamp_multilabel_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seasonal_svm_downsamp_multilabel_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TheFluShot_H1N1: Single Label Modelling, output H1N1 vaccine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single Label Modelling, output H1N1 vaccine -> Seasonal Flu Vaccine not in features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cat_features_no_vacc and X_no_vacc variables and the preprocessor remain the same from the multilabel modelling:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the target variable:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_h1n1_vacc = df.h1n1_vaccine"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#y_h1n1_vacc = y_h1n1_vacc.to_numpy()\n",
    "#y_h1n1_vacc = y_h1n1_vacc.squeeze()\n",
    "y_h1n1_vacc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing test-train split:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_train, X_no_vacc_test, y_h1n1_vacc_train, y_h1n1_vacc_test = train_test_split(X_no_vacc, y_h1n1_vacc, stratify = y_h1n1_vacc, test_size=0.2, random_state=RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('X_no_vacc_train shape:', X_no_vacc_train.shape)\n",
    "print('X_no_vacc_test shape:', X_no_vacc_test.shape)\n",
    "print('y_h1n1_vacc_train:', y_h1n1_vacc_train.shape)\n",
    "print('y_h1n1_vacc_test:', y_h1n1_vacc_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the pipeline for each model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", logreg),\n",
    "])\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", knn),\n",
    "])\n",
    "\n",
    "# for Random Forest\n",
    "\n",
    "rand_forst_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", rand_forst),\n",
    "])\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", svm),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data Balancing\n",
    "\n",
    "https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18\n",
    "\n",
    "Since the H1N1 label is unbalanced, three approaches will be tried to balance it:\n",
    "\n",
    "1. Oversampling of the minority class (h1n1_vaccine == 1)\n",
    "2. Undersampling of the majority class (h1n1_vaccine == 0)\n",
    "3. Creation of synthetic samples using SMOTE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating concatenated training dataframe and separating into minority and majority class  (for data balancing)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenating our train data back together\n",
    "\n",
    "concat_train_df = pd.concat([X_no_vacc_train, y_h1n1_vacc_train], axis = 1)\n",
    "\n",
    "# separating into minority and majority classes\n",
    "\n",
    "# majority class\n",
    "no_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==0]\n",
    "\n",
    "# minority class\n",
    "yes_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Oversampling of the minority class (upsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#upsample minority class\n",
    "yes_h1n1_vacc_upsampled = resample(yes_h1n1_vacc,\n",
    "                                   replace = True,\n",
    "                                   n_samples = len(no_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine majority and upsampled minority\n",
    "\n",
    "upsampled = pd.concat([no_h1n1_vacc, yes_h1n1_vacc_upsampled])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "upsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the upsampled data as train dataset\n",
    "\n",
    "y_h1n1_vacc_upsamp_train = upsampled.h1n1_vaccine\n",
    "\n",
    "X_no_vacc_upsamp_train =upsampled.drop('h1n1_vaccine', axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_upsamp_train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Undersampling of majority class (downsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# downsample minority class\n",
    "no_h1n1_vacc_downsampled = resample(no_h1n1_vacc,\n",
    "                                   replace = False,\n",
    "                                   n_samples = len(yes_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine minority and downsampled majority\n",
    "\n",
    "downsampled = pd.concat([no_h1n1_vacc_downsampled, yes_h1n1_vacc])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "downsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the downsampled data as train dataset\n",
    "\n",
    "y_h1n1_vacc_downsamp_train = downsampled.h1n1_vaccine\n",
    "\n",
    "X_no_vacc_downsamp_train =downsampled.drop('h1n1_vaccine', axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**3. Generate synthetic samples (SMOTE)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "SMOTE doesn't work with missing values and was therefore not trialled.\n",
    "If using SMOTE in the future--note that it doesn't work with string data and therefore some columns need to be encoded to numbers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training and predictions (run predictions immediately after training the model!)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc = logreg_unilabel_pipeline.fit(X_no_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc = knn_unilabel_pipeline.fit(X_no_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc = rand_forst_unilabel_pipeline.fit(X_no_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_no_vacc = svm_unilabel_pipeline.fit(X_no_vacc_train, y_h1n1_vacc_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_trainpreds = logreg_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "logreg_unilabel_no_vacc_testpreds = logreg_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_trainpreds = knn_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "knn_unilabel_no_vacc_testpreds = knn_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_trainpreds = rand_forst_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "rand_forst_unilabel_no_vacc_testpreds = rand_forst_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_no_vacc_trainpreds = svm_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "svm_unilabel_no_vacc_testpreds = svm_unilabel_no_vacc.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampled data--fitting model: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_upsamp = logreg_unilabel_pipeline.fit(X_no_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_upsamp = knn_unilabel_pipeline.fit(X_no_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_upsamp = rand_forst_unilabel_pipeline.fit(X_no_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_no_vacc_upsamp = svm_unilabel_pipeline.fit(X_no_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampled data--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_upsamp_trainpreds = logreg_unilabel_no_vacc_upsamp.predict(X_no_vacc_train)\n",
    "logreg_unilabel_no_vacc_upsamp_testpreds = logreg_unilabel_no_vacc_upsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_upsamp_trainpreds = knn_unilabel_no_vacc_upsamp.predict(X_no_vacc_train)\n",
    "knn_unilabel_no_vacc_upsamp_testpreds = knn_unilabel_no_vacc_upsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_upsamp_trainpreds = rand_forst_unilabel_no_vacc_upsamp.predict(X_no_vacc_train)\n",
    "rand_forst_unilabel_no_vacc_upsamp_testpreds = rand_forst_unilabel_no_vacc_upsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_no_vacc_upsamp_trainpreds = svm_unilabel_no_vacc_upsamp.predict(X_no_vacc_train)\n",
    "svm_unilabel_no_vacc_upsamp_testpreds = svm_unilabel_no_vacc_upsamp.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampled data--fitting the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_downsamp = logreg_unilabel_pipeline.fit(X_no_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_downsamp = knn_unilabel_pipeline.fit(X_no_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_downsamp = rand_forst_unilabel_pipeline.fit(X_no_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_no_vacc_downsamp = svm_unilabel_pipeline.fit(X_no_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampled data--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_downsamp_trainpreds = logreg_unilabel_no_vacc_downsamp.predict(X_no_vacc_train)\n",
    "logreg_unilabel_no_vacc_downsamp_testpreds = logreg_unilabel_no_vacc_downsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_downsamp_trainpreds = knn_unilabel_no_vacc_downsamp.predict(X_no_vacc_train)\n",
    "knn_unilabel_no_vacc_downsamp_testpreds = knn_unilabel_no_vacc_downsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_downsamp_trainpreds = rand_forst_unilabel_no_vacc_downsamp.predict(X_no_vacc_train)\n",
    "rand_forst_unilabel_no_vacc_downsamp_testpreds = rand_forst_unilabel_no_vacc_downsamp.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_no_vacc_downsamp_trainpreds = svm_unilabel_no_vacc_downsamp.predict(X_no_vacc_train)\n",
    "svm_unilabel_no_vacc_downsamp_testpreds = svm_unilabel_no_vacc_downsamp.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figure out later what this does and if we want to use it\n",
    "\n",
    "\n",
    "#y_train_predicted = cross_val_predict(full_pipeline, X_train, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train data--downsampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_no_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_no_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_no_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_no_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test data--downsampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_no_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_no_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_no_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_no_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_no_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_no_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_no_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_no_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking the model with MLFlow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### H1N1 vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_no_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_no_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_no_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_no_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_no_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_no_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_no_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_no_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_no_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_no_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_no_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_no_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_no_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_no_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_no_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_no_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_no_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_no_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_no_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_no_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_no_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_no_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_no_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_no_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_no_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_no_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_no_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_no_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_no_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_no_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_no_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_no_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_no_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_no_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_no_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_no_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_no_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_no_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single Label Modelling, output H1N1 vaccine -> Seasonal Flu Vaccine is in features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The y_h1n1_vacc remains the same from the previous model; the X feature and cat_features (for the preprocessor) need to be adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_seas_vacc = cat_features.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_seas_vacc.remove('h1n1_vaccine')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#NB: dropping the 'h1n1_vaccine' and 'seasonal_vaccine' columns\n",
    "X_seas_vacc = df.drop(columns=['h1n1_vaccine'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_seas_vacc.columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing test-train split (the same data can be used for each model in multilabelling):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_seas_vacc_train, X_seas_vacc_test, y_h1n1_vacc_train, y_h1n1_vacc_test = train_test_split(X_seas_vacc, y_h1n1_vacc, stratify = y_h1n1_vacc, test_size=0.2, random_state=RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('X_seas_vacc_train shape:', X_seas_vacc_train.shape)\n",
    "print('X_seas_vacc_test shape:', X_seas_vacc_test.shape)\n",
    "print('y_h1n1_vacc_train:', y_h1n1_vacc_train.shape)\n",
    "print('y_h1n1_vacc_test:', y_h1n1_vacc_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preprocessor is adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessor_seas_vacc = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, cat_features_seas_vacc)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pipeline is adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_seas_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_seas_vacc),\n",
    "    (\"estimators\", logreg),\n",
    "])\n",
    "\n",
    "# for KNN\n",
    "knn_seas_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_seas_vacc),\n",
    "    (\"estimators\", knn),\n",
    "])\n",
    "\n",
    "# for Random Forest\n",
    "\n",
    "rand_forst_seas_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_seas_vacc),\n",
    "    (\"estimators\", rand_forst),\n",
    "])\n",
    "\n",
    "#for SVM\n",
    "svm_seas_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_seas_vacc),\n",
    "    (\"estimators\", svm),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Creating concatenated training dataframe and separating into minority and majority class (for data balancing)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# concatenating our train data back together\n",
    "\n",
    "concat_train_df = pd.concat([X_seas_vacc_train, y_h1n1_vacc_train], axis = 1)\n",
    "\n",
    "# separating into minority and majority classes\n",
    "\n",
    "# majority class\n",
    "no_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==0]\n",
    "\n",
    "# minority class\n",
    "yes_h1n1_vacc = concat_train_df[concat_train_df.h1n1_vaccine==1]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1. Oversampling of the minority class (upsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#upsample minority class\n",
    "yes_h1n1_vacc_upsampled = resample(yes_h1n1_vacc,\n",
    "                                   replace = True,\n",
    "                                   n_samples = len(no_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine majority and upsampled minority\n",
    "\n",
    "upsampled = pd.concat([no_h1n1_vacc, yes_h1n1_vacc_upsampled])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "upsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the upsampled data as train dataset\n",
    "\n",
    "y_h1n1_vacc_upsamp_train = upsampled.h1n1_vaccine\n",
    "\n",
    "X_seas_vacc_upsamp_train =upsampled.drop('h1n1_vaccine', axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_seas_vacc_upsamp_train.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2. Undersampling of majority class (downsampling)**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# downsample minority class\n",
    "no_h1n1_vacc_downsampled = resample(no_h1n1_vacc,\n",
    "                                   replace = False,\n",
    "                                   n_samples = len(yes_h1n1_vacc),\n",
    "                                   random_state = RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# combine minority and downsampled majority\n",
    "\n",
    "downsampled = pd.concat([no_h1n1_vacc_downsampled, yes_h1n1_vacc])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# checking new class counts\n",
    "downsampled.h1n1_vaccine.value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# relabelling the downsampled data as train dataset\n",
    "\n",
    "y_h1n1_vacc_downsamp_train = downsampled.h1n1_vaccine\n",
    "\n",
    "X_seas_vacc_downsamp_train =downsampled.drop('h1n1_vaccine', axis = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training and predictions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc = logreg_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc = knn_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc = rand_forst_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_train, y_h1n1_vacc_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_seas_vacc = svm_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_train, y_h1n1_vacc_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "No resampling--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc_trainpreds = logreg_unilabel_seas_vacc.predict(X_seas_vacc_train)\n",
    "logreg_unilabel_seas_vacc_testpreds = logreg_unilabel_seas_vacc.predict(X_seas_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc_trainpreds = knn_unilabel_seas_vacc.predict(X_seas_vacc_train)\n",
    "knn_unilabel_seas_vacc_testpreds = knn_unilabel_seas_vacc.predict(X_seas_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc_trainpreds = rand_forst_unilabel_seas_vacc.predict(X_seas_vacc_train)\n",
    "rand_forst_unilabel_seas_vacc_testpreds = rand_forst_unilabel_seas_vacc.predict(X_seas_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_seas_vacc_trainpreds = svm_unilabel_seas_vacc.predict(X_seas_vacc_train)\n",
    "svm_unilabel_seas_vacc_testpreds = svm_unilabel_seas_vacc.predict(X_seas_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampled data--fitting model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc_upsamp = logreg_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc_upsamp = knn_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc_upsamp = rand_forst_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_seas_vacc_upsamp = svm_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_upsamp_train, y_h1n1_vacc_upsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Upsampled data--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc_upsamp_trainpreds = logreg_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_train)\n",
    "logreg_unilabel_seas_vacc_upsamp_testpreds = logreg_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc_upsamp_trainpreds = knn_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_train)\n",
    "knn_unilabel_seas_vacc_upsamp_testpreds = knn_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc_upsamp_trainpreds = rand_forst_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_train)\n",
    "rand_forst_unilabel_seas_vacc_upsamp_testpreds = rand_forst_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_seas_vacc_upsamp_trainpreds = svm_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_train)\n",
    "svm_unilabel_seas_vacc_upsamp_testpreds = svm_unilabel_seas_vacc_upsamp.predict(X_seas_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampled data--fitting the model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc_downsamp = logreg_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc_downsamp = knn_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc_downsamp = rand_forst_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_seas_vacc_downsamp = svm_seas_vacc_unilabel_pipeline.fit(X_seas_vacc_downsamp_train, y_h1n1_vacc_downsamp_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downsampled data--making predictions:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_seas_vacc_downsamp_trainpreds = logreg_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_train)\n",
    "logreg_unilabel_seas_vacc_downsamp_testpreds = logreg_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_seas_vacc_downsamp_trainpreds = knn_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_train)\n",
    "knn_unilabel_seas_vacc_downsamp_testpreds = knn_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_seas_vacc_downsamp_trainpreds = rand_forst_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_train)\n",
    "rand_forst_unilabel_seas_vacc_downsamp_testpreds = rand_forst_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_seas_vacc_downsamp_trainpreds = svm_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_train)\n",
    "svm_unilabel_seas_vacc_downsamp_testpreds = svm_unilabel_seas_vacc_downsamp.predict(X_seas_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figure out later what this does and if we want to use it\n",
    "\n",
    "\n",
    "#y_train_predicted = cross_val_predict(full_pipeline, X_train, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--no resampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_upsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_upsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data--upsampling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_upsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_upsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_upsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train data--downsampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, logreg_unilabel_seas_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, knn_unilabel_seas_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, rand_forst_unilabel_seas_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_train_acc = accuracy_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_train_recall = recall_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_train_precision = precision_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_train_f1 = f1_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_downsamp_trainpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_train_roc = roc_auc_score(y_h1n1_vacc_train, svm_unilabel_seas_vacc_downsamp_trainpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test data--downsampling**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_logreg_unilabel_seas_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, logreg_unilabel_seas_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_knn_unilabel_seas_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, knn_unilabel_seas_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, rand_forst_unilabel_seas_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_test_acc = accuracy_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_test_recall = recall_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_test_precision = precision_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_test_f1 = f1_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_downsamp_testpreds)\n",
    "h1n1_svm_unilabel_seas_vacc_downsamp_test_roc = roc_auc_score(y_h1n1_vacc_test, svm_unilabel_seas_vacc_downsamp_testpreds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking the model with MLFlow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### H1N1 vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_logreg_unilabel_seas_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_logreg_unilabel_seas_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_logreg_unilabel_seas_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_logreg_unilabel_seas_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_logreg_unilabel_seas_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'knn_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_knn_unilabel_seas_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_knn_unilabel_seas_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_knn_unilabel_seas_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_knn_unilabel_seas_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_knn_unilabel_seas_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'rand_forst_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_rand_forst_unilabel_seas_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*No resampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Upsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Upsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_upsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_upsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_upsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_upsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_upsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_upsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_upsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_upsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_upsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_upsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Downsampling*"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_h1n1)\n",
    "name = 'svm_unilabel_h1n1' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()\n",
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"Downsampling\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"Seasonal\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_downsamp_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", h1n1_svm_unilabel_seas_vacc_downsamp_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_downsamp_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", h1n1_svm_unilabel_seas_vacc_downsamp_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_downsamp_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", h1n1_svm_unilabel_seas_vacc_downsamp_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_downsamp_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", h1n1_svm_unilabel_seas_vacc_downsamp_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_downsamp_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", h1n1_svm_unilabel_seas_vacc_downsamp_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TheFluShot_seasonal: Single Label Modelling, output seasonal vaccine"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single Label Modelling, output seasonal vaccine -> H1N1 Flu Vaccine not in features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cat_features_no_vacc and X_no_vacc variables and the preprocessor remain the same from the multilabel modelling:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up the target variable:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_seas_vacc = df['seasonal_vaccine'].copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_seas_vacc = y_seas_vacc.to_numpy()\n",
    "y_seas_vacc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(y_seas_vacc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing test-train split:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_no_vacc_train, X_no_vacc_test, y_seas_vacc_train, y_seas_vacc_test = train_test_split(X_no_vacc, y_seas_vacc, stratify = y_seas_vacc, test_size=0.2, random_state=RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('X_no_vacc_train shape:', X_no_vacc_train.shape)\n",
    "print('X_no_vacc_test shape:', X_no_vacc_test.shape)\n",
    "print('y_seas_vacc_train:', y_seas_vacc_train.shape)\n",
    "print('y_seas_vacc_test:', y_seas_vacc_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting up the pipeline for each model:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", logreg),\n",
    "])\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", knn),\n",
    "])\n",
    "\n",
    "# for Random Forest\n",
    "\n",
    "rand_forst_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", rand_forst),\n",
    "])\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"estimators\", svm),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the models:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc = logreg_unilabel_pipeline.fit(X_no_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc = knn_unilabel_pipeline.fit(X_no_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc = rand_forst_unilabel_pipeline.fit(X_no_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_no_vacc = svm_unilabel_pipeline.fit(X_no_vacc_train, y_seas_vacc_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figure out later what this does and if we want to use it\n",
    "\n",
    "\n",
    "#y_train_predicted = cross_val_predict(full_pipeline, X_train, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making predictions based on train and test data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_no_vacc_trainpreds = logreg_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "logreg_unilabel_no_vacc_testpreds = logreg_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_no_vacc_trainpreds = knn_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "knn_unilabel_no_vacc_testpreds = knn_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_no_vacc_trainpreds = rand_forst_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "rand_forst_unilabel_no_vacc_testpreds = rand_forst_unilabel_no_vacc.predict(X_no_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_no_vacc_trainpreds = svm_unilabel_no_vacc.predict(X_no_vacc_train)\n",
    "svm_unilabel_no_vacc_testpreds = svm_unilabel_no_vacc.predict(X_no_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "seas_logreg_unilabel_no_vacc_train_acc = accuracy_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "seas_logreg_unilabel_no_vacc_train_recall = recall_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "seas_logreg_unilabel_no_vacc_train_precision = precision_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "seas_logreg_unilabel_no_vacc_train_f1 = f1_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "seas_logreg_unilabel_no_vacc_train_roc = roc_auc_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, logreg_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "seas_knn_unilabel_no_vacc_train_acc = accuracy_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "seas_knn_unilabel_no_vacc_train_recall = recall_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "seas_knn_unilabel_no_vacc_train_precision = precision_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "seas_knn_unilabel_no_vacc_train_f1 = f1_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "seas_knn_unilabel_no_vacc_train_roc = roc_auc_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, knn_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "seas_rand_forst_unilabel_no_vacc_train_acc = accuracy_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_train_recall = recall_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_train_precision = precision_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_train_f1 = f1_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_train_roc = roc_auc_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, rand_forst_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "seas_svm_unilabel_no_vacc_train_acc = accuracy_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "seas_svm_unilabel_no_vacc_train_recall = recall_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "seas_svm_unilabel_no_vacc_train_precision = precision_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "seas_svm_unilabel_no_vacc_train_f1 = f1_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "seas_svm_unilabel_no_vacc_train_roc = roc_auc_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, svm_unilabel_no_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "seas_logreg_unilabel_no_vacc_test_acc = accuracy_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "seas_logreg_unilabel_no_vacc_test_recall = recall_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "seas_logreg_unilabel_no_vacc_test_precision = precision_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "seas_logreg_unilabel_no_vacc_test_f1 = f1_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "seas_logreg_unilabel_no_vacc_test_roc = roc_auc_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, logreg_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "seas_knn_unilabel_no_vacc_test_acc = accuracy_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "seas_knn_unilabel_no_vacc_test_recall = recall_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "seas_knn_unilabel_no_vacc_test_precision = precision_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "seas_knn_unilabel_no_vacc_test_f1 = f1_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "seas_knn_unilabel_no_vacc_test_roc = roc_auc_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)))\n",
    "print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, knn_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(seas_knn_unilabel_no_vacc_test_acc)\n",
    "print(seas_knn_unilabel_no_vacc_test_recall)\n",
    "print(seas_knn_unilabel_no_vacc_test_precision)\n",
    "print(seas_knn_unilabel_no_vacc_test_f1)\n",
    "print(seas_knn_unilabel_no_vacc_test_roc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "seas_rand_forst_unilabel_no_vacc_test_acc = accuracy_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_test_recall = recall_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_test_precision = precision_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_test_f1 = f1_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_no_vacc_test_roc = roc_auc_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))\n",
    "print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, rand_forst_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(seas_rand_forst_unilabel_no_vacc_test_acc)\n",
    "print(seas_rand_forst_unilabel_no_vacc_test_recall)\n",
    "print(seas_rand_forst_unilabel_no_vacc_test_precision)\n",
    "print(seas_rand_forst_unilabel_no_vacc_test_f1)\n",
    "print(seas_rand_forst_unilabel_no_vacc_test_roc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data evaluation Metrics\n",
    "seas_svm_unilabel_no_vacc_test_acc = accuracy_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "seas_svm_unilabel_no_vacc_test_recall = recall_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "seas_svm_unilabel_no_vacc_test_precision = precision_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "seas_svm_unilabel_no_vacc_test_f1 = f1_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "seas_svm_unilabel_no_vacc_test_roc = roc_auc_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, svm_unilabel_no_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking the model with MLFlow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Seasonal vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_logreg_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_logreg_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_logreg_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_logreg_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_logreg_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_logreg_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_logreg_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_logreg_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_logreg_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_logreg_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'knn_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_knn_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_knn_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_knn_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_knn_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_knn_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_knn_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_knn_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_knn_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_knn_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_knn_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'rand_forst_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_rand_forst_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_rand_forst_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_rand_forst_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_rand_forst_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_rand_forst_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_rand_forst_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_rand_forst_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_rand_forst_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_rand_forst_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_rand_forst_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'svm_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"None\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_svm_unilabel_no_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_svm_unilabel_no_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_svm_unilabel_no_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_svm_unilabel_no_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_svm_unilabel_no_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_svm_unilabel_no_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_svm_unilabel_no_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_svm_unilabel_no_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_svm_unilabel_no_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_svm_unilabel_no_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Single Label Modelling, output seasonal vaccine -> H1N1 Flu Vaccine is in features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The y_seas_vacc remains the same from the previous model; the X feature and cat_features (for the preprocessor) need to be adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_h1n1_vacc = cat_features.copy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cat_features_h1n1_vacc.remove('seasonal_vaccine')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#NB: dropping the 'h1n1_vaccine' and 'seasonal_vaccine' columns\n",
    "X_h1n1_vacc = df.drop(columns=['seasonal_vaccine'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Performing test-train split (the same data can be used for each model in multilabelling):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_h1n1_vacc_train, X_h1n1_vacc_test, y_seas_vacc_train, y_seas_vacc_test = train_test_split(X_h1n1_vacc, y_seas_vacc, stratify = y_seas_vacc, test_size=0.2, random_state=RSEED)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('X_h1n1_vacc_train shape:', X_h1n1_vacc_train.shape)\n",
    "print('X_h1n1_vacc_test shape:', X_h1n1_vacc_test.shape)\n",
    "print('y_seas_vacc_train:', y_seas_vacc_train.shape)\n",
    "print('y_seas_vacc_test:', y_seas_vacc_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The preprocessor is adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preprocessor_h1n1_vacc = ColumnTransformer([\n",
    "    ('cat', cat_pipeline, cat_features_h1n1_vacc)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The pipeline is adjusted:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_h1n1_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_h1n1_vacc),\n",
    "    (\"estimators\", logreg),\n",
    "])\n",
    "\n",
    "# for KNN\n",
    "knn_h1n1_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_h1n1_vacc),\n",
    "    (\"estimators\", knn),\n",
    "])\n",
    "\n",
    "# for Random Forest\n",
    "\n",
    "rand_forst_h1n1_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_h1n1_vacc),\n",
    "    (\"estimators\", rand_forst),\n",
    "])\n",
    "\n",
    "#for SVM\n",
    "svm_h1n1_vacc_unilabel_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor_h1n1_vacc),\n",
    "    (\"estimators\", svm),\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the models:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_h1n1_vacc = logreg_h1n1_vacc_unilabel_pipeline.fit(X_h1n1_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_h1n1_vacc = knn_h1n1_vacc_unilabel_pipeline.fit(X_h1n1_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_h1n1_vacc = rand_forst_h1n1_vacc_unilabel_pipeline.fit(X_h1n1_vacc_train, y_seas_vacc_train)\n",
    "\n",
    "#for SVM\n",
    "svm_unilabel_h1n1_vacc = svm_h1n1_vacc_unilabel_pipeline.fit(X_h1n1_vacc_train, y_seas_vacc_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Figure out later what this does and if we want to use it\n",
    "\n",
    "\n",
    "#y_train_predicted = cross_val_predict(full_pipeline, X_train, y_train, cv=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Making predictions based on train and test data:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for logreg\n",
    "logreg_unilabel_h1n1_vacc_trainpreds = logreg_unilabel_h1n1_vacc.predict(X_h1n1_vacc_train)\n",
    "logreg_unilabel_h1n1_vacc_testpreds = logreg_unilabel_h1n1_vacc.predict(X_h1n1_vacc_test)\n",
    "\n",
    "# for KNN\n",
    "knn_unilabel_h1n1_vacc_trainpreds = knn_unilabel_h1n1_vacc.predict(X_h1n1_vacc_train)\n",
    "knn_unilabel_h1n1_vacc_testpreds = knn_unilabel_h1n1_vacc.predict(X_h1n1_vacc_test)\n",
    "\n",
    "# for Random Forest\n",
    "rand_forst_unilabel_h1n1_vacc_trainpreds = rand_forst_unilabel_h1n1_vacc.predict(X_h1n1_vacc_train)\n",
    "rand_forst_unilabel_h1n1_vacc_testpreds = rand_forst_unilabel_h1n1_vacc.predict(X_h1n1_vacc_test)\n",
    "\n",
    "# for SVM\n",
    "svm_unilabel_h1n1_vacc_trainpreds = svm_unilabel_h1n1_vacc.predict(X_h1n1_vacc_train)\n",
    "svm_unilabel_h1n1_vacc_testpreds = svm_unilabel_h1n1_vacc.predict(X_h1n1_vacc_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Train data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Train data evaluation Metrics\n",
    "seas_logreg_unilabel_h1n1_vacc_train_acc = accuracy_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_train_recall = recall_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_train_precision = precision_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_train_f1 = f1_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_train_roc = roc_auc_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, logreg_unilabel_h1n1_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Train data evaluation Metrics\n",
    "seas_knn_unilabel_h1n1_vacc_train_acc = accuracy_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_train_recall = recall_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_train_precision = precision_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_train_f1 = f1_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_train_roc = roc_auc_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, knn_unilabel_h1n1_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Train data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "seas_rand_forst_unilabel_h1n1_vacc_train_acc = accuracy_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_train_recall = recall_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_train_precision = precision_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_train_f1 = f1_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_train_roc = roc_auc_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, rand_forst_unilabel_h1n1_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Train data evaluation Metrics\n",
    "seas_svm_unilabel_h1n1_vacc_train_acc = accuracy_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_train_recall = recall_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_train_precision = precision_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_train_f1 = f1_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_train_roc = roc_auc_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_train, svm_unilabel_h1n1_vacc_trainpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Logreg--Test data evaluation Metrics\n",
    "seas_logreg_unilabel_h1n1_vacc_test_acc = accuracy_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_test_recall = recall_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_test_precision = precision_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_test_f1 = f1_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)\n",
    "seas_logreg_unilabel_h1n1_vacc_test_roc = roc_auc_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, logreg_unilabel_h1n1_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN--Test data evaluation Metrics\n",
    "seas_knn_unilabel_h1n1_vacc_test_acc = accuracy_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_test_recall = recall_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_test_precision = precision_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_test_f1 = f1_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)\n",
    "seas_knn_unilabel_h1n1_vacc_test_roc = roc_auc_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, knn_unilabel_h1n1_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Random Forest--Test data evaluation Metrics\n",
    "#check if these metrics are changing compared to previous results\n",
    "seas_rand_forst_unilabel_h1n1_vacc_test_acc = accuracy_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_test_recall = recall_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_test_precision = precision_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_test_f1 = f1_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)\n",
    "seas_rand_forst_unilabel_h1n1_vacc_test_roc = roc_auc_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, rand_forst_unilabel_h1n1_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# SVM--Test data evaluation Metrics\n",
    "seas_svm_unilabel_h1n1_vacc_test_acc = accuracy_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_test_recall = recall_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_test_precision = precision_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_test_f1 = f1_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)\n",
    "seas_svm_unilabel_h1n1_vacc_test_roc = roc_auc_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)\n",
    "\n",
    "#print(\"Accuracy: {:.2f}\".format(accuracy_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Recall: {:.2f}\".format(recall_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"Precision: {:.2f}\".format(precision_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"F1: {:.2f}\".format(f1_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)))\n",
    "#print(\"ROC: {:.2f}\".format(roc_auc_score(y_seas_vacc_test, svm_unilabel_h1n1_vacc_testpreds)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tracking the model with MLFlow"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### H1N1 vaccine output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Logistic regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal) #this needs to be adjusted for each experiment\n",
    "name = 'logreg_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"H1N1\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_logreg_unilabel_h1n1_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_logreg_unilabel_h1n1_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_logreg_unilabel_h1n1_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_logreg_unilabel_h1n1_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_logreg_unilabel_h1n1_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_logreg_unilabel_h1n1_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_logreg_unilabel_h1n1_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_logreg_unilabel_h1n1_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_logreg_unilabel_h1n1_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_logreg_unilabel_h1n1_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### KNN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'knn_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"H1N1\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_knn_unilabel_h1n1_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_knn_unilabel_h1n1_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_knn_unilabel_h1n1_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_knn_unilabel_h1n1_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_knn_unilabel_h1n1_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_knn_unilabel_h1n1_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_knn_unilabel_h1n1_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_knn_unilabel_h1n1_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_knn_unilabel_h1n1_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_knn_unilabel_h1n1_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Random forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'rand_forst_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"H1N1\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_rand_forst_unilabel_h1n1_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_rand_forst_unilabel_h1n1_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_rand_forst_unilabel_h1n1_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_rand_forst_unilabel_h1n1_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_rand_forst_unilabel_h1n1_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_rand_forst_unilabel_h1n1_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_rand_forst_unilabel_h1n1_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_rand_forst_unilabel_h1n1_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_rand_forst_unilabel_h1n1_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_rand_forst_unilabel_h1n1_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# setting the MLFlow connection and experiment\n",
    "mlflow.set_tracking_uri(TRACKING_URI)\n",
    "mlflow.set_experiment(EXPERIMENT_NAME_seasonal)\n",
    "name = 'svm_unilabel_seasonal' #specify the run name here; name it with model used\n",
    "mlflow.start_run(run_name = name) \n",
    "run = mlflow.active_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Active run_id: {}\".format(run.info.run_id))"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#parameters are to keep track of everything for re-running the experiment (e.g. what features are being engineered, hyperparameters of model, etc)\n",
    "#to be adjusted as needed\n",
    "params = {\n",
    "    \"Data cleaning\": \"Full dataset\",\n",
    "    \"Data balancing\": \"None\",\n",
    "    \"Hyperparameters\": \"None\"\n",
    "  }"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mlflow.log_params(params)\n",
    "#tags = data used\n",
    "mlflow.set_tag(\"Vaccines in features\", \"H1N1\") #set tags for more details of what we've done\n",
    "mlflow.log_metric(\"train -\" + \"ROC\", seas_svm_unilabel_h1n1_vacc_train_roc)\n",
    "mlflow.log_metric(\"test -\" + \"ROC\", seas_svm_unilabel_h1n1_vacc_test_roc)\n",
    "mlflow.log_metric(\"train -\" + \"accuracy\", seas_svm_unilabel_h1n1_vacc_train_acc)\n",
    "mlflow.log_metric(\"test -\" + \"accuracy\", seas_svm_unilabel_h1n1_vacc_test_acc)\n",
    "mlflow.log_metric(\"train -\" + \"recall\", seas_svm_unilabel_h1n1_vacc_train_recall)\n",
    "mlflow.log_metric(\"test -\" + \"recall\", seas_svm_unilabel_h1n1_vacc_test_recall)\n",
    "mlflow.log_metric(\"train -\" + \"precision\", seas_svm_unilabel_h1n1_vacc_train_precision)\n",
    "mlflow.log_metric(\"test -\" + \"precision\", seas_svm_unilabel_h1n1_vacc_test_precision)\n",
    "mlflow.log_metric(\"train -\" + \"f1\", seas_svm_unilabel_h1n1_vacc_train_f1)\n",
    "mlflow.log_metric(\"test -\" + \"f1\", seas_svm_unilabel_h1n1_vacc_test_f1)\n",
    "#mlflow.log_artifact(\"../models\")\n",
    "#mlflow.sklearn.log_model(reg, \"model\")\n",
    "mlflow.end_run()"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#mlflow.get_run(run_id=run.info.run_id)"
   ],
   "outputs": [],
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bdc0c931d2d4c1e254c9b407be81fb640f6c2f032b4d6ab78d09bb2272bcd9af"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('.venv')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}