{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c622f5",
   "metadata": {},
   "source": [
    "# Flu shot acceptance: learning from a previous pandemic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae624809",
   "metadata": {},
   "source": [
    "This is the capstone project of the neue fische Data Science Bootcamp. It was a collaborative effort between [Christina Rudolf](https://github.com/christinarudolf), [Raymond Boateng](https://github.com/RayKwame) and [Juliane Berek](https://github.com/julianeberek).\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The H1N1 influenza pandemic (also known as swine influenza) was caused by H1N1 influenza virus and affected the world between June 2009 to August 2010 (according to WHO declaration). It was the most recent pandemic prior to COVID19. An estimated 11 - 21% of the global population was affected, with deaths in the U.S. totalling 12,469.\n",
    "\n",
    "This project aimed to:\n",
    "- Help to increase vaccination rates for seasonal and pandemic flu in the overall population (this reducing the burden of influenza by decreasing hospitalisations/ deaths)\n",
    "- Identify factors that determine the chance of getting vaccinated\n",
    "- Identify groups with lower likelihood for getting vaccinated, in order to target them with promotions\n",
    "- Determine differences of seasonal vs. H1N1 (pandemic) vaccinations\n",
    "\n",
    "## Description of dataset\n",
    "\n",
    "The original data was collected through the National 2009 H1N1 Flu Survey in the U.S. between 2009 - 2010. The current dataset formed part of the DrivenData Challenge [\"Flu Shot Learning: Predict H1N1 and Seasonal Flu Vaccines\"](https://www.drivendata.org/competitions/66/flu-shot-learning/).\n",
    "\n",
    "The dataset consists of approximately 27 000 participant responses. The main outcome variables in the dataset:\n",
    "- whether the participant received the vaccination against the H1N1 flu\n",
    "- whether the participant received the vaccination against the seasonal flu\n",
    "\n",
    "The remainder of the dataset consists of [35 categorical variables](https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/), broadly falling into participant demographics, attitudes and knowledge about H1N1 and seasonal flu and vaccination, and healthcare information.\n",
    "\n",
    "## Hypotheses\n",
    "\n",
    "We had two main hypotheses we intended to address in our project:\n",
    "\n",
    "- Some features affect the likelihood of vaccination more than others, e.g. attitudes and knowledge, recommendations by doctors\n",
    "- H1N1 vaccination is taken more due to the pandemic context "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab961f",
   "metadata": {},
   "source": [
    "## Notebook description\n",
    "\n",
    "![](https://raw.githubusercontent.com/RayKwame/TheFluShot/main/images/Model_creation.JPG?token=ATDLUYIYB7MTMIFTWAQWZS3BCFWO6)\n",
    "This notebook describes the steps taken to create the final prediction models for predicting vaccination against the flu (for the H1N1 and seasonal flu respectively). It also describes the identification and exploration of the important features for vaccination prediction.\n",
    "\n",
    "It includes the following sections:\n",
    "\n",
    "1. Import of necessary packages and libraries\n",
    "2. Import and examination of data\n",
    "3. Data cleaning\n",
    "4. EDA\n",
    "5. Feature engineering\n",
    "6. Prediction of H1N1 flu vaccination\n",
    "    6.1 Data balancing by downsampling the majority class\n",
    "    6.2 Modelling and prediction\n",
    "    6.3 Evaluation of model performance\n",
    "    6.4 Hyperparameter tuning\n",
    "    6.5 Error evaluation of model\n",
    "7. Prediction of seasonal flu vaccination\n",
    "    7.1 Data balancing by downsampling the majority class\n",
    "    7.2 Modelling and prediction\n",
    "    7.3 Evaluation of model performance\n",
    "    7.4 Hyperparameter tuning\n",
    "    7.5 Error evaluation of model\n",
    "8. Summary and conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2285ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eli5\n",
      "  Downloading eli5-0.11.0-py2.py3-none-any.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 3.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting graphviz\n",
      "  Downloading graphviz-0.17-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.8/site-packages (from eli5) (1.16.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.8/site-packages (from eli5) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in ./.venv/lib/python3.8/site-packages (from eli5) (0.24.2)\n",
      "Requirement already satisfied: attrs>16.0.0 in ./.venv/lib/python3.8/site-packages (from eli5) (21.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in ./.venv/lib/python3.8/site-packages (from eli5) (1.20.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in ./.venv/lib/python3.8/site-packages (from eli5) (0.8.9)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.8/site-packages (from eli5) (3.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in ./.venv/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.venv/lib/python3.8/site-packages (from scikit-learn>=0.20->eli5) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.8/site-packages (from jinja2->eli5) (2.0.1)\n",
      "Installing collected packages: graphviz, eli5\n",
      "Successfully installed eli5-0.11.0 graphviz-0.17\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# adding to the path variables the one folder higher (locally, not changing system variables)\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import mlflow\n",
    "#from modeling.config import TRACKING_URI, EXPERIMENT_NAME\n",
    "\n",
    "RSEED = 42\n",
    "# Modeling Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, cross_validate\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# for resampling (data balancing)\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# for feature importance\n",
    "!pip install eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e577a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Flu_Shot_Data_cleaned_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0914a479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 26707 entries, 0 to 26706\n",
      "Data columns (total 38 columns):\n",
      " #   Column                       Non-Null Count  Dtype  \n",
      "---  ------                       --------------  -----  \n",
      " 0   Unnamed: 0                   26707 non-null  int64  \n",
      " 1   h1n1_vaccine                 26707 non-null  int64  \n",
      " 2   seasonal_vaccine             26707 non-null  int64  \n",
      " 3   h1n1_concern                 26615 non-null  float64\n",
      " 4   h1n1_knowledge               26591 non-null  float64\n",
      " 5   behavioral_antiviral_meds    26636 non-null  float64\n",
      " 6   behavioral_avoidance         26499 non-null  float64\n",
      " 7   behavioral_face_mask         26688 non-null  float64\n",
      " 8   behavioral_wash_hands        26665 non-null  float64\n",
      " 9   behavioral_large_gatherings  26620 non-null  float64\n",
      " 10  behavioral_outside_home      26625 non-null  float64\n",
      " 11  behavioral_touch_face        26579 non-null  float64\n",
      " 12  doctor_recc_h1n1             24547 non-null  float64\n",
      " 13  doctor_recc_seasonal         24547 non-null  float64\n",
      " 14  chronic_med_condition        25736 non-null  float64\n",
      " 15  child_under_6_months         25887 non-null  float64\n",
      " 16  health_worker                25903 non-null  float64\n",
      " 17  health_insurance             14433 non-null  float64\n",
      " 18  opinion_h1n1_vacc_effective  26316 non-null  float64\n",
      " 19  opinion_h1n1_risk            26319 non-null  float64\n",
      " 20  opinion_h1n1_sick_from_vacc  26312 non-null  float64\n",
      " 21  opinion_seas_vacc_effective  26245 non-null  float64\n",
      " 22  opinion_seas_risk            26193 non-null  float64\n",
      " 23  opinion_seas_sick_from_vacc  26170 non-null  float64\n",
      " 24  age_group                    26707 non-null  object \n",
      " 25  education                    25300 non-null  object \n",
      " 26  race                         26707 non-null  object \n",
      " 27  sex                          26707 non-null  object \n",
      " 28  income_poverty               22284 non-null  object \n",
      " 29  marital_status               25299 non-null  object \n",
      " 30  rent_or_own                  24665 non-null  object \n",
      " 31  employment_status            25244 non-null  object \n",
      " 32  hhs_geo_region               26707 non-null  object \n",
      " 33  census_msa                   26707 non-null  object \n",
      " 34  household_adults             26458 non-null  float64\n",
      " 35  household_children           26458 non-null  float64\n",
      " 36  employment_industry          13377 non-null  object \n",
      " 37  employment_occupation        13237 non-null  object \n",
      "dtypes: float64(23), int64(3), object(12)\n",
      "memory usage: 7.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69e0ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
